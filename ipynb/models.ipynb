{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import IPython.display\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from random import shuffle\n",
    "import keras\n",
    "import traceback\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from keras.utils import np_utils\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from multiprocessing import Process\n",
    "import traceback\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def read_png(path):\n",
    "    img = mpimg.imread(path)\n",
    "    return rgb2gray(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature File Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramFeatureFileManager(object):\n",
    "    def __init__(self, seconds=10, skip_augment=False):\n",
    "        self.seconds = seconds\n",
    "        self.skip_augment = skip_augment\n",
    "\n",
    "    def get_file_credentials(self, path):\n",
    "        values = path.split('#')[-1].split('.')[0]\n",
    "        sr, winlen, winstep = map(int, values.split('='))\n",
    "        return sr, winlen, winstep\n",
    "    \n",
    "    def _load_file(self, path):\n",
    "        return read_png(path)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\" Return list of objects, ectracted from given file \"\"\"\n",
    "        if self.skip_augment and u'#AUG#' in path:\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            file_data = self._load_file(path)\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "        sr, winlen, winstep = self.get_file_credentials(path)\n",
    "        if winlen < winstep:\n",
    "            winlen, winstep = winstep, winlen\n",
    "\n",
    "        one_sec_count = sr / winlen\n",
    "        chunk_size = one_sec_count * self.seconds\n",
    "\n",
    "        height, width = file_data.shape\n",
    "        \n",
    "        # print file_data.shape, width, sr, winlen, winstep\n",
    "        # print one_sec_count, chunk_size\n",
    "        \n",
    "        if width < chunk_size:\n",
    "            return []\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        chunks = (width + chunk_size - 1) / chunk_size\n",
    "        for chunk in xrange(chunks):\n",
    "            start = (chunk * chunk_size)\n",
    "            end = min(file_data.shape[1], (chunk + 1) * chunk_size)\n",
    "            \n",
    "            if end - start < chunk_size:\n",
    "                break\n",
    "\n",
    "            result.append(file_data[:, start:end])\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MFCCFeatureFileManager(SpectrogramFeatureFileManager):\n",
    "#     def get_file_credentials(self, path):\n",
    "#         values = path.split('#')[-1].split('.')[0]\n",
    "#         sr, winlen, winstep, _, _, _, _ = map(int, values.split('='))\n",
    "#         return sr, winlen, winstep\n",
    "    \n",
    "#     def _load_file(self, path):\n",
    "#         return np.loadtxt(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FolderIterator(object):\n",
    "    PATH_TEMPLATE = '/home/kolegor/Study/Master/data/features/{feature}/{language}/{dataset}/'\n",
    "\n",
    "    def __init__(self, language, dataset, feature, uid, max_count=None):\n",
    "        self.language = language\n",
    "        self.dataset = dataset\n",
    "        self.uid = uid\n",
    "        self.max_count = max_count\n",
    "\n",
    "        self.path = self.PATH_TEMPLATE.format(feature=feature, language=language, dataset=dataset)\n",
    "    \n",
    "    def get_data(self):\n",
    "        x = [os.path.join(self.path, filename) for filename in os.listdir(self.path)]\n",
    "        y = [self.uid for _ in xrange(len(x))]\n",
    "        \n",
    "        shuffle(x)\n",
    "        shuffle(y)\n",
    "        \n",
    "        if self.max_count:\n",
    "            x = x[:self.max_count]\n",
    "            y = y[:self.max_count]\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, folder_iterators, need_shuffle=True, manager=None, max_count=None):\n",
    "        self.folders = folder_iterators\n",
    "        self.manager = manager\n",
    "        \n",
    "        self.size_by_language = dict()\n",
    "        \n",
    "        self.all_x = []\n",
    "        self.all_y = []\n",
    "        \n",
    "        for folder in self.folders:\n",
    "            x, y = folder.get_data()\n",
    "            if len(x) != len(y):\n",
    "                raise Exception('What the hell?')\n",
    "            \n",
    "            self.all_x.extend(x)\n",
    "            self.all_y.extend(y)\n",
    "            self.size_by_language[folder.language] = len(x)\n",
    "        \n",
    "        if shuffle:\n",
    "            combined = zip(self.all_x, self.all_y)\n",
    "            shuffle(combined)\n",
    "            self.all_x, self.all_y = zip(*combined)\n",
    "        \n",
    "        if max_count is not None:\n",
    "            self.all_x = self.all_x[:max_count]\n",
    "            self.all_y = self.all_y[:max_count]\n",
    "\n",
    "        self.classes = len(set(self.all_y))\n",
    "        self.size = len(self.all_x)\n",
    "\n",
    "    def one_sample(self):\n",
    "        qwe = []\n",
    "        while not qwe:\n",
    "            qwe = self.manager.load(self.all_x[0])\n",
    "            if qwe:\n",
    "                qwe = qwe[0]\n",
    "                break\n",
    "        return qwe\n",
    "\n",
    "\n",
    "def dataset_iterator(dataset, chunk_size, as_np=False, extend=True, y_categorical=True, iters=10):\n",
    "    for _ in xrange(iters):\n",
    "        cur_chunk_x = []\n",
    "        cur_chunk_y = []\n",
    "\n",
    "        for filepath, uid in zip(dataset.all_x, dataset.all_y):\n",
    "            loaded = dataset.manager.load(filepath)\n",
    "\n",
    "            cur_chunk_x.extend(loaded)\n",
    "            cur_chunk_y.extend([uid for _ in xrange(len(loaded))])\n",
    "\n",
    "            if len(cur_chunk_x) >= chunk_size:\n",
    "                to_return_x = cur_chunk_x[:chunk_size]\n",
    "                to_return_y = cur_chunk_y[:chunk_size]\n",
    "\n",
    "                cur_chunk_x = cur_chunk_x[chunk_size:]\n",
    "                cur_chunk_y = cur_chunk_y[chunk_size:]\n",
    "\n",
    "                if as_np:\n",
    "                    to_return_x = np.array(to_return_x)\n",
    "                    to_return_y = np.array(to_return_y)\n",
    "\n",
    "                if extend:\n",
    "                    to_return_x = to_return_x.reshape(to_return_x.shape + (1,))\n",
    "\n",
    "                if y_categorical:\n",
    "                    to_return_y = np_utils.to_categorical(to_return_y, dataset.classes)\n",
    "\n",
    "                yield to_return_x, to_return_y\n",
    "\n",
    "\n",
    "def load_all_dataset(dataset, as_np=False, extend=True, y_categorical=True):\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "    \n",
    "    iterator = dataset_iterator(dataset, 32, as_np=False, extend=False, y_categorical=False, iters=1)\n",
    "    for x, y in iterator:\n",
    "        all_x.extend(x)\n",
    "        all_y.extend(y)\n",
    "        \n",
    "    all_x = np.array(all_x)\n",
    "    all_y = np.array(all_y)\n",
    "    \n",
    "    if as_np:\n",
    "        all_x = np.array(all_x)\n",
    "        all_y = np.array(all_y)\n",
    "\n",
    "    if extend:\n",
    "        all_x = all_x.reshape(all_x.shape + (1,))\n",
    "\n",
    "    if y_categorical:\n",
    "        all_y = np_utils.to_categorical(all_y, dataset.classes)\n",
    "\n",
    "    return all_x, all_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_languages_dataset(feature, languages, max_per_class=(None, None, None)):\n",
    "    lang2uid = dict((b, a) for a, b in enumerate(languages))\n",
    "    train = Dataset([\n",
    "        FolderIterator(language, 'train', feature, lang2uid[language], max_count=max_per_class[0])\n",
    "        for language in languages\n",
    "    ])\n",
    "    test = Dataset([\n",
    "        FolderIterator(language, 'test', feature, lang2uid[language], max_count=max_per_class[1])\n",
    "        for language in languages\n",
    "    ])\n",
    "    dev = Dataset([\n",
    "        FolderIterator(language, 'dev', feature, lang2uid[language], max_count=max_per_class[2])\n",
    "        for language in languages\n",
    "    ])\n",
    "    return train, test, dev, lang2uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_languages_info(train_ds, test_ds, dev_ds):\n",
    "    print u'Languages: [{}]'.format(u' # '.join(train_ds.size_by_language.keys()))\n",
    "    print u'Classes: {}'.format(train_ds.classes)\n",
    "    print u'Sizes: (train/test/dev): {}/{}/{}'.format(train_ds.size, test_ds.size, dev_ds.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(object):\n",
    "    def __init__(\n",
    "        self, feature, languages, manager,\n",
    "        batch_size=32, max_per_class=(None, None, None), uid=None, full_load=False\n",
    "    ):\n",
    "        self.batch_size = 32\n",
    "        self.max_per_class = max_per_class\n",
    "        self.uid = uid\n",
    "        self.full_load = full_load\n",
    "\n",
    "        self.train_ds = None\n",
    "        self.test_ds = None\n",
    "        self.dev_ds = None\n",
    "        self.lang2uid = None\n",
    "        \n",
    "        self.train_ds, self.test_ds, self.dev_ds, self.lang2uid = get_languages_dataset(\n",
    "            feature,\n",
    "            languages,\n",
    "            self.max_per_class\n",
    "        )\n",
    "        print_languages_info(self.train_ds, self.test_ds, self.dev_ds)\n",
    "        \n",
    "        self.train_ds.manager = manager\n",
    "        self.test_ds.manager = manager\n",
    "        self.dev_ds.manager = manager\n",
    "        \n",
    "        self.X_train, self.Y_train = None, None\n",
    "        self.X_test, self.Y_test = None, None\n",
    "        self.X_dev, self.Y_dev = None, None\n",
    "        \n",
    "        if full_load:\n",
    "            self.X_train, self.Y_train = load_all_dataset(self.train_ds, as_np=True)\n",
    "            self.X_test, self.Y_test = load_all_dataset(self.test_ds, as_np=True)\n",
    "            self.X_dev, self.Y_dev = load_all_dataset(self.dev_ds, as_np=True)\n",
    "            \n",
    "            print u'Full load. Train: {}, Test: {}, Dev: {}'.format(\n",
    "                len(self.X_train), len(self.X_test), len(self.X_dev)\n",
    "            )\n",
    "        \n",
    "        self.model = None\n",
    "        self.fit_result = None\n",
    "        self.evaluate_result = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNExperiment(Experiment):    \n",
    "    def init_model(self):\n",
    "        if self.full_load:\n",
    "            sample = self.X_train[0]\n",
    "        else:\n",
    "            sample = self.train_ds.one_sample()\n",
    "        \n",
    "        input_shape = (sample.shape[0], sample.shape[1], 1)\n",
    "        print input_shape\n",
    "\n",
    "        pool_size=(2, 2)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(16, (7, 7), activation='relu', input_shape=input_shape))\n",
    "        model.add(MaxPooling2D(pool_size=pool_size))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(32, (5, 5), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=pool_size))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=pool_size))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=pool_size))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=pool_size))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        # print model.summary()\n",
    "        # model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "        # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(self.train_ds.classes, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def fit(self, epochs=4):\n",
    "        if self.full_load:\n",
    "            self.fit_result = self.model.fit(\n",
    "                x=self.X_train,\n",
    "                y=self.Y_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=self.batch_size,\n",
    "                validation_data=(self.X_dev, self.Y_dev),\n",
    "            )\n",
    "        else:\n",
    "            self.fit_result = self.model.fit_generator(\n",
    "                dataset_iterator(self.train_ds, self.batch_size, as_np=True, extend=True),\n",
    "                steps_per_epoch=len(self.train_ds.all_x) / self.batch_size,\n",
    "                epochs=epochs,\n",
    "                validation_data=dataset_iterator(self.dev_ds, self.batch_size, as_np=True, extend=True),\n",
    "                validation_steps=len(self.dev_ds.all_x) / self.batch_size,\n",
    "            )\n",
    "            \n",
    "    \n",
    "    def evaluate(self):\n",
    "        if self.full_load:\n",
    "            self.evaluate_result = self.model.evaluate(\n",
    "                x=self.X_test,\n",
    "                y=self.X_test,\n",
    "            )\n",
    "        else:\n",
    "            self.evaluate_result = self.model.evaluate_generator(\n",
    "                dataset_iterator(self.test_ds, self.batch_size, as_np=True, extend=True),\n",
    "                steps=len(self.test_ds.all_x) / self.batch_size\n",
    "            )\n",
    "    \n",
    "    def save(self, uid=None):\n",
    "        use_uid = uid\n",
    "        if use_uid is None:\n",
    "            use_uid = self.uid\n",
    "        if use_uid is None:\n",
    "            raise Exception('UID ex empty')\n",
    "        \n",
    "        with open('/home/kolegor/result.{}.pickle'.format(use_uid), 'w') as outf:\n",
    "            pickle.dump([self.fit_result.history, self.evaluate_result], outf)\n",
    "        \n",
    "        self.model.save('/home/kolegor/model.{}.pickle'.format(use_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(exp, uid=None, cleanup=False):\n",
    "    try:\n",
    "        print u'\\nINIT MODEL'\n",
    "        exp.init_model()\n",
    "\n",
    "        print u'\\nFIT'\n",
    "        exp.fit()\n",
    "\n",
    "        print u'\\nEVALUATE'\n",
    "        exp.evaluate()\n",
    "\n",
    "        print u'\\nSAVE'\n",
    "        exp.save(uid=uid)\n",
    "\n",
    "        if cleanup:\n",
    "            del exp.model\n",
    "            del exp.train_ds\n",
    "            del exp.test_ds\n",
    "            del exp.dev_ds\n",
    "            del exp.X_train, exp.Y_train, exp.X_test, exp.Y_test, exp.X_dev, exp.Y_dev\n",
    "    except:\n",
    "        print traceback.format_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp0_manager = SpectrogramFeatureFileManager(seconds=5, skip_augment=False)\n",
    "# exp0_languages = ['sa_afrikaans', 'nepali']\n",
    "# exp0 = CNNExperiment('spectrogram', exp0_languages, exp0_manager, max_per_class=(200, 100, 500), full_load=True)\n",
    "# exp0.uid = u'cnn__spectr__{}__6000_2000_500__5sec__aug'.format(u' # '.join(exp0_languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: [ab_Russian # ab_English]\n",
      "Classes: 2\n",
      "Sizes: (train/test/dev): 26829/10563/2786\n"
     ]
    }
   ],
   "source": [
    "exp1_manager = SpectrogramFeatureFileManager(seconds=9, skip_augment=False)\n",
    "exp1_languages = ['ab_English', 'ab_Russian']\n",
    "exp1 = CNNExperiment('spectrogram', exp1_languages, exp1_manager, max_per_class=(20000, 10000, 5000))\n",
    "exp1.uid = u'cnn__spectr__{}__20000_10000_5000__9sec__aug'.format(u'#'.join(exp1_languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: [ab_Russian # ab_English]\n",
      "Classes: 2\n",
      "Sizes: (train/test/dev): 26829/10563/2786\n"
     ]
    }
   ],
   "source": [
    "exp2_manager = SpectrogramFeatureFileManager(seconds=9, skip_augment=True)\n",
    "exp2_languages = ['ab_English', 'ab_Russian']\n",
    "exp2 = CNNExperiment('spectrogram', exp2_languages, exp2_manager, max_per_class=(20000, 10000, 5000))\n",
    "exp2.uid = u'cnn__spectr__{}__20000_10000_5000__9sec__NO_aug'.format(u'#'.join(exp2_languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: [khmer # sa_afrikaans # nepali # sa_sesotho]\n",
      "Classes: 4\n",
      "Sizes: (train/test/dev): 22013/6290/2000\n"
     ]
    }
   ],
   "source": [
    "exp3_manager = SpectrogramFeatureFileManager(seconds=2, skip_augment=False)\n",
    "exp3_languages = ['sa_afrikaans', 'khmer', 'nepali', 'sa_sesotho',]\n",
    "exp3 = CNNExperiment('spectrogram', exp3_languages, exp3_manager, max_per_class=(7000, 2000, 500))\n",
    "exp3.uid = u'cnn__spectr__{}__7000_2000_500__2sec__aug'.format(u'#'.join(exp3_languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: [khmer # sa_afrikaans # nepali # sa_sesotho]\n",
      "Classes: 4\n",
      "Sizes: (train/test/dev): 22013/6290/2000\n"
     ]
    }
   ],
   "source": [
    "exp4_manager = SpectrogramFeatureFileManager(seconds=5, skip_augment=False)\n",
    "exp4_languages = ['sa_afrikaans', 'khmer', 'nepali', 'sa_sesotho']\n",
    "exp4 = CNNExperiment('spectrogram', exp4_languages, exp4_manager, max_per_class=(7000, 2000, 500))\n",
    "exp4.uid = u'cnn__spectr__{}__7000_2000_500__5sec__aug'.format(u'#'.join(exp4_languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: [khmer # sa_afrikaans # nepali # sa_sesotho]\n",
      "Classes: 4\n",
      "Sizes: (train/test/dev): 22013/6290/2000\n"
     ]
    }
   ],
   "source": [
    "exp5_manager = SpectrogramFeatureFileManager(seconds=3, skip_augment=True)\n",
    "exp5_languages = ['sa_afrikaans', 'khmer', 'nepali', 'sa_sesotho']\n",
    "exp5 = CNNExperiment('spectrogram', exp5_languages, exp5_manager, max_per_class=(7000, 2000, 500))\n",
    "exp5.uid = u'cnn__spectr__{}__7000_2000_500__3sec__NO_aug'.format(u'#'.join(exp5_languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: [ab_Poland # ab_Finnish # ab_Hebrew # ab_English # ab_German # ab_Russian]\n",
      "Classes: 6\n",
      "Sizes: (train/test/dev): 36000/15000/6000\n"
     ]
    }
   ],
   "source": [
    "exp6_manager = SpectrogramFeatureFileManager(seconds=5, skip_augment=False)\n",
    "exp6_languages = ['ab_English', 'ab_Russian', 'ab_Hebrew', 'ab_German', 'ab_Poland', 'ab_Finnish']\n",
    "exp6 = CNNExperiment('spectrogram', exp6_languages, exp6_manager, max_per_class=(6000, 2500, 1000))\n",
    "exp6.uid = u'cnn__spectr__{}__5000_2500_1000__5sec__aug'.format(u'#'.join(exp6_languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: [ab_Russian # ab_German # ab_Hebrew # ab_English # ab_Finnish]\n",
      "Classes: 5\n",
      "Sizes: (train/test/dev): 30000/12500/5000\n"
     ]
    }
   ],
   "source": [
    "exp7_manager = SpectrogramFeatureFileManager(seconds=5, skip_augment=False)\n",
    "exp7_languages = ['ab_English', 'ab_Russian', 'ab_Hebrew', 'ab_German', 'ab_Finnish']\n",
    "exp7 = CNNExperiment('spectrogram', exp7_languages, exp7_manager, max_per_class=(6000, 2500, 1000))\n",
    "exp7.uid = u'cnn__spectr__{}__5000_2500_1000__5sec__aug'.format(u'#'.join(exp7_languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: [ab_Russian # ab_German # ab_Hebrew # ab_English # ab_Finnish]\n",
      "Classes: 5\n",
      "Sizes: (train/test/dev): 30000/12500/5000\n"
     ]
    }
   ],
   "source": [
    "exp8_manager = SpectrogramFeatureFileManager(seconds=7, skip_augment=False)\n",
    "exp8_languages = ['ab_English', 'ab_Russian', 'ab_Hebrew', 'ab_German', 'ab_Finnish']\n",
    "exp8 = CNNExperiment('spectrogram', exp8_languages, exp8_manager, max_per_class=(6000, 2500, 1000))\n",
    "exp8.uid = u'cnn__spectr__{}__5000_2500_1000__7sec__aug'.format(u'#'.join(exp8_languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INIT MODEL\n",
      "(128, 200, 1)\n",
      "\n",
      "FIT\n",
      "Epoch 1/4\n",
      "687/687 [==============================] - 739s 1s/step - loss: 0.1953 - acc: 0.9304 - val_loss: 0.7409 - val_acc: 0.8548\n",
      "Epoch 2/4\n",
      "687/687 [==============================] - 714s 1s/step - loss: 0.0504 - acc: 0.9819 - val_loss: 0.7284 - val_acc: 0.8327\n",
      "Epoch 3/4\n",
      "686/687 [============================>.] - ETA: 1s - loss: 0.0368 - acc: 0.9882Traceback (most recent call last):\n",
      "  File \"<ipython-input-11-281bb86d6846>\", line 7, in run_experiment\n",
      "    exp.fit()\n",
      "  File \"<ipython-input-10-550c02a52a86>\", line 58, in fit\n",
      "    validation_steps=len(self.dev_ds.all_x) / self.batch_size,\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/training.py\", line 1415, in fit_generator\n",
      "    initial_epoch=initial_epoch)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/training_generator.py\", line 230, in fit_generator\n",
      "    workers=0)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/training.py\", line 1469, in evaluate_generator\n",
      "    verbose=verbose)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/training_generator.py\", line 327, in evaluate_generator\n",
      "    generator_output = next(output_generator)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/keras/utils/data_utils.py\", line 785, in get\n",
      "    raise StopIteration()\n",
      "StopIteration\n",
      "\n",
      "\n",
      "INIT MODEL\n",
      "(128, 280, 1)\n",
      "\n",
      "FIT\n",
      "Epoch 1/4\n",
      "937/937 [==============================] - 1411s 2s/step - loss: 0.4074 - acc: 0.8576 - val_loss: 1.6116 - val_acc: 0.5112\n",
      "Epoch 2/4\n",
      "937/937 [==============================] - 1412s 2s/step - loss: 0.1607 - acc: 0.9442 - val_loss: 3.1993 - val_acc: 0.4573\n",
      "Epoch 3/4\n",
      "937/937 [==============================] - 1416s 2s/step - loss: 0.1096 - acc: 0.9621 - val_loss: 3.0359 - val_acc: 0.5327\n",
      "Epoch 4/4\n",
      "937/937 [==============================] - 1416s 2s/step - loss: 0.0855 - acc: 0.9703 - val_loss: 2.3941 - val_acc: 0.5799\n",
      "\n",
      "EVALUATE\n",
      "\n",
      "SAVE\n"
     ]
    }
   ],
   "source": [
    "run_experiment(exp4, cleanup=True)\n",
    "# run_experiment(exp2, cleanup=True)\n",
    "# run_experiment(exp3, cleanup=True)\n",
    "# run_experiment(exp6, cleanup=True)\n",
    "# run_experiment(exp7, cleanup=True)\n",
    "run_experiment(exp8, cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/home/kolegor/result.cnn__spectr__ab_English#ab_Russian#ab_Hebrew#ab_German#ab_Finnish__5000_2500_1000__3sec__aug.pickle'\n",
    "path2 = '/home/kolegor/result.cnn__spectr__ab_English#ab_Russian#ab_Hebrew#ab_German#ab_Finnish__5000_2500_1000__5sec__aug.pickle'\n",
    "path3 = '/home/kolegor/result.cnn__spectr__ab_English#ab_Russian#ab_Hebrew#ab_German#ab_Finnish__5000_2500_1000__7sec__aug.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'acc': [0.7474986659551761,\n",
       "   0.9545424226254002,\n",
       "   0.9713513874066169,\n",
       "   0.9802561366061899],\n",
       "  'loss': [0.6101389460070824,\n",
       "   0.13508696147584104,\n",
       "   0.08417417824103347,\n",
       "   0.060734387023224014],\n",
       "  'val_acc': [0.5612980769230769,\n",
       "   0.6426282051282052,\n",
       "   0.6225961538461539,\n",
       "   0.6546474358974359],\n",
       "  'val_loss': [2.521544595559438,\n",
       "   2.344950178112739,\n",
       "   2.7127186999871182,\n",
       "   3.1981816553534608]},\n",
       " [3.457260944904425, 0.6766826923076923]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_path = path3\n",
    "with open(use_path, 'r') as inf:\n",
    "    qwe = pickle.load(inf)\n",
    "qwe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'/home/kolegor/result.cnn__spectr__ab_English#ab_Russian#ab_Hebrew#ab_German#ab_Finnish__5000_2500_1000__5sec__aug.pickle'\n",
    "\n",
    "[{'acc': [0.6573505869797225,\n",
    "   0.9305296157950907,\n",
    "   0.9543756670224119,\n",
    "   0.9617796157950907],\n",
    "  'loss': [0.8304459522612464,\n",
    "   0.20158600124576215,\n",
    "   0.13396440932730694,\n",
    "   0.11191276282100891],\n",
    "  'val_acc': [0.6614583333333334,\n",
    "   0.6149839743589743,\n",
    "   0.6508413461538461,\n",
    "   0.6598557692307693],\n",
    "  'val_loss': [2.3027218018586817,\n",
    "   2.7794124357020245,\n",
    "   2.3347269101784778,\n",
    "   2.52298259587051]},\n",
    " [2.7109747937808817, 0.6872596153846153]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_dataset_data(dataset, max_count, manager=None):    \n",
    "    x = []\n",
    "    \n",
    "    for i, filepath in enumerate(dataset.all_x):\n",
    "        if i % 500 == 0:\n",
    "            print ' -', i, max_count, len(x)\n",
    "\n",
    "        loaded = dataset.manager.load(filepath)\n",
    "        x.extend(loaded)\n",
    "        \n",
    "        if len(x) > max_count:\n",
    "            break\n",
    "    \n",
    "    x = x[:max_count]\n",
    "    return np.vstack(x)\n",
    "\n",
    "\n",
    "def load_gmm_data(language, dataset, max_count, manager):\n",
    "    dataset = Dataset([FolderIterator(language, dataset, 'mfcc', -1, max_count=None)])\n",
    "    dataset.manager = manager\n",
    "    return get_language_dataset_data(dataset, max_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_gmm_models(languages, components):\n",
    "    model_by_language = dict()\n",
    "    for language in languages:\n",
    "        gmm = GaussianMixture(n_components=components, init_params='random', tol=1e-3, max_iter=50)\n",
    "        model_by_language[language] = gmm\n",
    "    return model_by_language\n",
    "\n",
    "\n",
    "def fit_all_gmm_models(model_by_language, components, manager, max_count):\n",
    "    for i, language in enumerate(model_by_language):\n",
    "        gmm = model_by_language[language]\n",
    "        \n",
    "        print ' - Load X {}/{}'.format(i + 1, len(model_by_language))\n",
    "        X = load_gmm_data(language, 'train', max_count, manager)\n",
    "        \n",
    "        print ' - Fit {}/{}'.format(i + 1, len(model_by_language))\n",
    "        gmm = gmm.fit(X)\n",
    "        del X\n",
    "        \n",
    "        model_by_language[language] = gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(languages, max_count, manager):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for language in languages:\n",
    "        lang_x = load_gmm_data(language, 'test', max_count, manager)\n",
    "        lang_y = [language for _ in xrange(len(lang_x))]\n",
    "        X.extend(lang_x)\n",
    "        Y.extend(lang_y)\n",
    "    \n",
    "    X = X[:max_count]\n",
    "    Y = Y[:max_count]\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def run_gmm_test(model_by_language, languages, max_count, manager):\n",
    "    print u'Loading TEST'\n",
    "    X_test, Y_test = load_test_data(languages, max_count, manager)\n",
    "    \n",
    "    predicted_by_language = dict()\n",
    "    for language, gmm in model_by_language.iteritems():\n",
    "        predicted_by_language[language] = gmm.score_samples(X_test)\n",
    "    \n",
    "    return Y_test, predicted_by_language\n",
    "\n",
    "\n",
    "def evaluate_predict_results(real, predicted_by_language):\n",
    "    results = []  # list of tuples (real_lang, [sorted predited_langs])\n",
    "    \n",
    "    for i, real_lang in enumerate(real):\n",
    "        predictions = [(pr[i], language) for language, pr in predicted_by_language.iteritems()]\n",
    "        best_val, best_lang = sorted(predictions)[-1]\n",
    "        results.append((real_lang, best_lang))\n",
    "    \n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    \n",
    "    for a, b in results:\n",
    "        if a == b:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    \n",
    "    return [results, correct, incorrect, len(real), float(correct) / len(real), float(incorrect) / len(real)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMMExperiment(object):\n",
    "    def __init__(self, languages, components, train_max_count, test_max_count, seconds=10, skip_augment=False):\n",
    "        self.manager = MFCCFeatureFileManager(seconds=seconds, skip_augment=skip_augment)\n",
    "        \n",
    "        self.languages = languages\n",
    "        self.components = components\n",
    "        self.train_max_count = train_max_count\n",
    "        self.test_max_count = test_max_count\n",
    "        \n",
    "        self.model_by_language = create_all_gmm_models(self.languages, self.components)\n",
    "        self.test_result = None\n",
    "        \n",
    "        self.model_uid = u'gmm#{}#{}#{}#{}#{}#{}'.format(\n",
    "            u'$'.join(self.languages), self.components, self.train_max_count, self.test_max_count,\n",
    "            self.manager.seconds, self.manager.skip_augment\n",
    "        )\n",
    "    \n",
    "    def fit(self):\n",
    "        fit_all_gmm_models(self.model_by_language, self.components, self.manager, self.train_max_count)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        real, predicted_by_language = run_gmm_test(\n",
    "            self.model_by_language,\n",
    "            self.languages,\n",
    "            self.test_max_count * len(self.languages),\n",
    "            self.manager\n",
    "        )\n",
    "        evaluated_results = evaluate_predict_results(real, predicted_by_language)\n",
    "        self.test_result = {'real': list(real), 'predicted': predicted_by_language, 'eval': evaluated_results}\n",
    "        print evaluated_results[1:]\n",
    "\n",
    "    def save(self):\n",
    "        with open('/home/kolegor/{}.model.pickle'.format(self.model_uid), 'wb') as outf:\n",
    "            pickle.dump(self.model_by_language, outf)\n",
    "        with open('/home/kolegor/{}.results.pickle'.format(self.model_uid), 'wb') as outf:\n",
    "            pickle.dump(self.test_result, outf)\n",
    "    \n",
    "    def cleanup(self):\n",
    "        del self.model_by_language\n",
    "        del self.test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gexp0 = GMMExperiment(['sa_isiXhosa', 'sa_sesotho'], 64, 3000, 600, seconds=4, skip_augment=False)\n",
    "# gexp1 = GMMExperiment(['sa_isiXhosa', 'sa_sesotho'], 256, 3000, 600, seconds=4, skip_augment=False)\n",
    "# gexp2 = GMMExperiment(['sa_isiXhosa', 'sa_sesotho'], 1024, 3000, 600, seconds=4, skip_augment=False)\n",
    "# gexp3 = GMMExperiment(['sa_isiXhosa', 'sa_sesotho'], 256, 3000, 600, seconds=4, skip_augment=False)\n",
    "# gexp4 = GMMExperiment(['sa_isiXhosa', 'sa_sesotho'], 1024, 3000, 600, seconds=4, skip_augment=True)\n",
    "# gexp5 = GMMExperiment(['sa_isiXhosa', 'sa_sesotho'], 256, 3000, 600, seconds=4, skip_augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_languages = ['ab_English', 'ab_German', 'ab_Russian', 'khmer', 'nepali', 'sa_afrikaans', 'sa_sesotho']\n",
    "gexp6 = GMMExperiment(many_languages, 256, 2000, 600, seconds=3, skip_augment=False)\n",
    "# gexp7 = GMMExperiment(many_languages, 2048, 2000, 500, seconds=3, skip_augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_languages = ['ab_English', 'ab_German', 'ab_Russian']\n",
    "# gexp8 = GMMExperiment(big_languages, 64, 3000, 500, seconds=9, skip_augment=False)\n",
    "# gexp9 = GMMExperiment(big_languages, 256, 3000, 500, seconds=9, skip_augment=False)\n",
    "# gexp10 = GMMExperiment(big_languages, 1024, 3000, 500, seconds=9, skip_augment=False)\n",
    "gexp11 = GMMExperiment(big_languages, 256, 3000, 700, seconds=9, skip_augment=False)\n",
    "# gexp12 = GMMExperiment(big_languages, 1024, 3000, 500, seconds=9, skip_augment=True)\n",
    "gexp13 = GMMExperiment(big_languages, 256, 3000, 700, seconds=9, skip_augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_languages_2 = ['ab_English', 'ab_German', 'ab_Russian', 'ab_Hebrew', 'ab_Danish']\n",
    "# gexp14 = GMMExperiment(big_languages_2, 64, 2000, 400, seconds=9, skip_augment=False)\n",
    "# gexp15 = GMMExperiment(big_languages_2, 256, 2000, 400, seconds=9, skip_augment=False)\n",
    "gexp16 = GMMExperiment(big_languages_2, 256, 2000, 600, seconds=9, skip_augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments = [\n",
    "    gexp6, gexp11, gexp13, gexp16\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_proc_experiments(exps, uid):\n",
    "    for i, exp in enumerate(exps):\n",
    "        print u'START. PROC {}. EXP {}/{}.'.format(uid, i + 1, len(exps))\n",
    "        \n",
    "        try:\n",
    "            print u' - FIT ({}, {}/{})'.format(uid, i + 1, len(exps))\n",
    "            exp.fit()\n",
    "            print u' - EVAL ({}, {}/{})'.format(uid, i + 1, len(exps))\n",
    "            exp.evaluate()\n",
    "            print u' - SAVE ({}, {}/{})'.format(uid, i + 1, len(exps))\n",
    "            exp.save()\n",
    "            exp.cleanup()\n",
    "        except:\n",
    "            print traceback.format_exc()\n",
    "            print u' * EXCEPTION. PROC {}. EXP {}/{}. SKIP'.format(uid, i + 1, len(exps))\n",
    "            continue\n",
    "    \n",
    "\n",
    "def run_experiments_procs(processes=4):\n",
    "    procs = []\n",
    "\n",
    "    chunk_size = (len(all_experiments) + processes - 1) / processes\n",
    "    for chunk in xrange(processes):\n",
    "        chunk_exps = all_experiments[chunk * chunk_size:min(len(all_experiments), (chunk + 1) * chunk_size)]\n",
    "\n",
    "        proc = Process(target=run_one_proc_experiments, args=(chunk_exps, chunk))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "\n",
    "    # complete the processes\n",
    "    for proc in procs:\n",
    "        proc.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START. PROC 0. EXP 1/4.\n",
      " - FIT (0, 1/4)\n",
      " - Load X 1/7\n",
      " - 0 2000 0\n",
      " - 500 2000 1164\n",
      " - Fit 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kolegor/.local/lib/python2.7/site-packages/sklearn/mixture/base.py:237: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Load X 2/7\n",
      " - 0 2000 0\n",
      " - Fit 2/7\n",
      " - Load X 3/7\n",
      " - 0 2000 0\n",
      " - 500 2000 1173\n",
      " - Fit 3/7\n",
      " - Load X 4/7\n",
      " - 0 2000 0\n",
      " - 500 2000 1333\n",
      " - Fit 4/7\n",
      " - Load X 5/7\n",
      " - 0 2000 0\n",
      " - Fit 5/7\n",
      " - Load X 6/7\n",
      " - 0 2000 0\n",
      " - 500 2000 888\n",
      " - 1000 2000 1784\n",
      " - Fit 6/7\n",
      " - Load X 7/7\n",
      " - 0 2000 0\n",
      " - Fit 7/7\n",
      " - EVAL (0, 1/4)\n",
      "Loading TEST\n",
      " - 0 4200 0\n",
      " - 500 4200 2464\n",
      " - 0 4200 0\n",
      " - 500 4200 2457\n",
      " - 0 4200 0\n",
      " - 500 4200 2497\n",
      " - 0 4200 0\n",
      " - 500 4200 1113\n",
      " - 1000 4200 2241\n",
      " - 1500 4200 3360\n",
      " - 0 4200 0\n",
      " - 500 4200 1241\n",
      " - 1000 4200 2480\n",
      " - 1500 4200 3748\n",
      " - 0 4200 0\n",
      " - 500 4200 964\n",
      " - 1000 4200 1902\n",
      " - 0 4200 0\n",
      " - 500 4200 1368\n",
      " - 1000 4200 2768\n",
      " - 1500 4200 4116\n",
      "[673, 3527, 4200, 0.16023809523809524, 0.8397619047619047]\n",
      " - SAVE (0, 1/4)\n",
      "START. PROC 0. EXP 2/4.\n",
      " - FIT (0, 2/4)\n",
      " - Load X 1/3\n",
      " - 0 3000 0\n",
      " - 500 3000 568\n",
      " - 1000 3000 1119\n",
      " - 1500 3000 1671\n",
      " - 2000 3000 2243\n",
      " - 2500 3000 2796\n",
      " - Fit 1/3\n",
      " - Load X 2/3\n",
      " - 0 3000 0\n",
      " - 500 3000 597\n",
      " - 1000 3000 1185\n",
      " - 1500 3000 1780\n",
      " - 2000 3000 2375\n",
      " - 2500 3000 2961\n",
      " - Fit 2/3\n",
      " - Load X 3/3\n",
      " - 0 3000 0\n",
      " - 500 3000 561\n",
      " - 1000 3000 1129\n",
      " - 1500 3000 1689\n",
      " - 2000 3000 2258\n",
      " - 2500 3000 2811\n",
      " - Fit 3/3\n",
      " - EVAL (0, 2/4)\n",
      "Loading TEST\n",
      " - 0 2100 0\n",
      " - 500 2100 564\n",
      " - 1000 2100 1113\n",
      " - 1500 2100 1662\n",
      " - 0 2100 0\n",
      " - 500 2100 597\n",
      " - 1000 2100 1201\n",
      " - 1500 2100 1814\n",
      " - 0 2100 0\n",
      " - 500 2100 564\n",
      " - 1000 2100 1123\n",
      " - 1500 2100 1692\n",
      "[732, 1368, 2100, 0.3485714285714286, 0.6514285714285715]\n",
      " - SAVE (0, 2/4)\n",
      "START. PROC 0. EXP 3/4.\n",
      " - FIT (0, 3/4)\n",
      " - Load X 1/3\n",
      " - 0 3000 0\n",
      " - 500 3000 246\n",
      " - 1000 3000 479\n",
      " - 1500 3000 723\n",
      " - 2000 3000 962\n",
      " - 2500 3000 1212\n",
      " - 3000 3000 1451\n",
      " - 3500 3000 1712\n",
      " - 4000 3000 1961\n",
      " - 4500 3000 2212\n",
      " - 5000 3000 2450\n",
      " - 5500 3000 2710\n",
      " - 6000 3000 2937\n",
      " - Fit 1/3\n",
      " - Load X 2/3\n",
      " - 0 3000 0\n",
      " - 500 3000 120\n",
      " - 1000 3000 265\n",
      " - 1500 3000 405\n",
      " - 2000 3000 528\n",
      " - 2500 3000 670\n",
      " - 3000 3000 790\n",
      " - 3500 3000 936\n",
      " - 4000 3000 1066\n",
      " - 4500 3000 1204\n",
      " - 5000 3000 1338\n",
      " - 5500 3000 1475\n",
      " - 6000 3000 1598\n",
      " - 6500 3000 1730\n",
      " - 7000 3000 1873\n",
      " - 7500 3000 1997\n",
      " - 8000 3000 2122\n",
      " - 8500 3000 2249\n",
      " - 9000 3000 2382\n",
      " - 9500 3000 2529\n",
      " - 10000 3000 2652\n",
      " - 10500 3000 2783\n",
      " - 11000 3000 2922\n",
      " - Fit 2/3\n",
      " - Load X 3/3\n",
      " - 0 3000 0\n",
      " - 500 3000 247\n",
      " - 1000 3000 488\n",
      " - 1500 3000 734\n",
      " - 2000 3000 979\n",
      " - 2500 3000 1230\n",
      " - 3000 3000 1489\n",
      " - 3500 3000 1752\n",
      " - 4000 3000 2008\n",
      " - 4500 3000 2250\n",
      " - 5000 3000 2503\n",
      " - 5500 3000 2724\n",
      " - 6000 3000 2971\n",
      " - Fit 3/3\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-52-cd7858913bff>\", line 7, in run_one_proc_experiments\n",
      "    exp.fit()\n",
      "  File \"<ipython-input-46-a70ba2927853>\", line 19, in fit\n",
      "    fit_all_gmm_models(self.model_by_language, self.components, self.manager, self.train_max_count)\n",
      "  File \"<ipython-input-44-f0c7fa2f26cb>\", line 17, in fit_all_gmm_models\n",
      "    gmm = gmm.fit(X)\n",
      "  File \"/home/kolegor/.local/lib/python2.7/site-packages/sklearn/mixture/base.py\", line 214, in fit\n",
      "    self._m_step(X, log_resp)\n",
      "  File \"/home/kolegor/.local/lib/python2.7/site-packages/sklearn/mixture/gaussian_mixture.py\", line 668, in _m_step\n",
      "    self.covariance_type))\n",
      "  File \"/home/kolegor/.local/lib/python2.7/site-packages/sklearn/mixture/gaussian_mixture.py\", line 285, in _estimate_gaussian_parameters\n",
      "    }[covariance_type](resp, X, nk, means, reg_covar)\n",
      "  File \"/home/kolegor/.local/lib/python2.7/site-packages/sklearn/mixture/gaussian_mixture.py\", line 167, in _estimate_gaussian_covariances_full\n",
      "    covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k]\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * EXCEPTION. PROC 0. EXP 3/4. SKIP\n",
      "START. PROC 0. EXP 4/4.\n",
      " - FIT (0, 4/4)\n",
      " - Load X 1/5\n",
      " - 0 2000 0\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-52-cd7858913bff>\", line 7, in run_one_proc_experiments\n",
      "    exp.fit()\n",
      "  File \"<ipython-input-46-a70ba2927853>\", line 19, in fit\n",
      "    fit_all_gmm_models(self.model_by_language, self.components, self.manager, self.train_max_count)\n",
      "  File \"<ipython-input-44-f0c7fa2f26cb>\", line 14, in fit_all_gmm_models\n",
      "    X = load_gmm_data(language, 'train', max_count, manager)\n",
      "  File \"<ipython-input-43-f6f2f0551502>\", line 21, in load_gmm_data\n",
      "    return get_language_dataset_data(dataset, max_count)\n",
      "  File \"<ipython-input-43-f6f2f0551502>\", line 8, in get_language_dataset_data\n",
      "    loaded = dataset.manager.load(filepath)\n",
      "  File \"<ipython-input-5-29d13f746c78>\", line 19, in load\n",
      "    file_data = self._load_file(path)\n",
      "  File \"<ipython-input-6-ef8856755d51>\", line 8, in _load_file\n",
      "    return np.loadtxt(path)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.py\", line 1092, in loadtxt\n",
      "    for x in read_data(_loadtxt_chunksize):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.py\", line 1007, in read_data\n",
      "    for i, line in enumerate(itertools.chain([first_line], fh)):\n",
      "  File \"/usr/lib/python2.7/codecs.py\", line 311, in decode\n",
      "    def decode(self, input, final=False):\n",
      "KeyboardInterrupt\n",
      "\n",
      " * EXCEPTION. PROC 0. EXP 4/4. SKIP\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kolegor/.local/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/kolegor/.local/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/kolegor/.local/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1051, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1011, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 453, in getsourcefile\n",
      "    if hasattr(getmodule(object, filename), '__loader__'):\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 490, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home/kolegor/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2893\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kolegor/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1824\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 1826\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kolegor/.local/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1411\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kolegor/.local/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1319\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m             )\n\u001b[1;32m   1321\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kolegor/.local/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m             \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructured_traceback_parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "run_experiments_procs(processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'acc': [0.9464139344262295,\n",
       "   0.9881147540983607,\n",
       "   0.9914617486338798,\n",
       "   0.9934084699453551],\n",
       "  'loss': [0.1342539332273971,\n",
       "   0.03442741199977832,\n",
       "   0.025367211781744257,\n",
       "   0.019217864393748806],\n",
       "  'val_acc': [0.6178030303030303,\n",
       "   0.6566287878787879,\n",
       "   0.46912878787878787,\n",
       "   0.6056818181818182],\n",
       "  'val_loss': [3.205211460229122,\n",
       "   3.1818838964809073,\n",
       "   3.7943293333053587,\n",
       "   4.1201699957703095]},\n",
       " [3.0749430269906015, 0.734469696969697]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path1 = '/home/kolegor/result.cnn__spectr__ab_English#ab_Russian__20000_10000_5000__9sec__aug.pickle'\n",
    "with open(path1, 'r') as inf:\n",
    "    data1 = pickle.load(inf)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'acc': [0.9546106557377049,\n",
       "   0.9942964480874317,\n",
       "   0.9967213114754099,\n",
       "   0.9974043715846994],\n",
       "  'loss': [0.12049935693097813,\n",
       "   0.01853571963003406,\n",
       "   0.009686795414060782,\n",
       "   0.008042469599610648],\n",
       "  'val_acc': [0.506439393939394,\n",
       "   0.5776515151515151,\n",
       "   0.5547348484848484,\n",
       "   0.5356060606060606],\n",
       "  'val_loss': [3.128516874891339,\n",
       "   3.791149545438362,\n",
       "   4.508427002935699,\n",
       "   4.903973020206799]},\n",
       " [4.127637905785532, 0.6269886363636363]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path2 = '/home/kolegor/result.cnn__spectr__ab_English#ab_Russian__20000_10000_5000__9sec__NO_aug.pickle'\n",
    "with open(path2, 'r') as inf:\n",
    "    data2 = pickle.load(inf)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'acc': [0.7020105531295487,\n",
       "   0.9098890101892285,\n",
       "   0.9388191411935953,\n",
       "   0.9513282387190685],\n",
       "  'loss': [0.6875336158375379,\n",
       "   0.23914713782883282,\n",
       "   0.1645430876891057,\n",
       "   0.12773855280922794],\n",
       "  'val_acc': [0.7777217741935484,\n",
       "   0.6875,\n",
       "   0.8150201612903226,\n",
       "   0.8568548387096774],\n",
       "  'val_loss': [0.5415891235874545,\n",
       "   0.8141414729818222,\n",
       "   0.5098349149188688,\n",
       "   0.42031823735563983]},\n",
       " [0.28748251347593506, 0.9033801020408163]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path3 = '/home/kolegor/result.cnn__spectr__sa_afrikaans#khmer#nepali#sa_sesotho__7000_2000_500__2sec__aug.pickle'\n",
    "with open(path3, 'r') as inf:\n",
    "    data3 = pickle.load(inf)\n",
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_gmm_results(real, predicted_by_language):\n",
    "    results = []  # list of tuples (real_lang, predicted_lang)\n",
    "    \n",
    "    for i, real_lang in enumerate(real):\n",
    "        predictions = [(pr[i], language) for language, pr in predicted_by_language.iteritems()]\n",
    "        best_val, best_lang = sorted(predictions)[-1]\n",
    "        results.append((real_lang, best_lang))\n",
    "    \n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    \n",
    "    for a, b in results:\n",
    "        if a == b:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    \n",
    "    return [results, correct, incorrect, len(real), float(correct) / len(real), float(incorrect) / len(real)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n"
     ]
    }
   ],
   "source": [
    "path4 = '/home/kolegor/gmm#sa_isiXhosa$sa_sesotho#512#3000#600#4#True.results.pickle'\n",
    "with open(path4, 'r') as inf:\n",
    "    data4 = pickle.load(inf)\n",
    "print len(data4['predicted']['sa_isiXhosa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print data4['eval']\n",
    "print mean_gmm_results(data4['real'], data4['predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "languages = ['ab_Portuguese', 'ab_Russian', 'ab_English', 'ab_French', 'ab_German']\n",
    "seconds = 5, skip_augment = False\n",
    "\n",
    "Epoch 1/4\n",
    "1419/1419 [==============================] - 5687s 4s/step - loss: 0.4233 - acc: 0.8370 - val_loss: 3.0405 - val_acc: 0.5762\n",
    "Epoch 2/4\n",
    "1419/1419 [==============================] - 5266s 4s/step - loss: 0.1159 - acc: 0.9614 - val_loss: 3.1389 - val_acc: 0.5513\n",
    "Epoch 3/4\n",
    "1419/1419 [==============================] - 5178s 4s/step - loss: 0.0742 - acc: 0.9752 - val_loss: 2.5753 - val_acc: 0.6113\n",
    "Epoch 4/4\n",
    "1419/1419 [==============================] - 5163s 4s/step - loss: 0.0564 - acc: 0.9814 - val_loss: 3.3037 - val_acc: 0.5399\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gexp0 = GMMExperiment(['sa_isiXhosa', 'sa_sesotho'], 64, 3000, 600, seconds=4, skip_augment=False)\n",
    "gexp1 = GMMExperiment(['sa_isiXhosa', 'sa_sesotho'], 256, 3000, 600, seconds=4, skip_augment=False)\n",
    "[743, 457, 1200, 0.6191666666666666, 0.38083333333333336]\n",
    "[800, 400, 1200, 0.6666666666666666, 0.3333333333333333]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gexp3 = GMMExperiment(['sa_isiXhosa', 'sa_sesotho'], 512, 3000, 600, seconds=4, skip_augment=False)\n",
    "[855, 345, 1200, 0.7125, 0.2875]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gexp5 = GMMExperiment(['sa_isiXhosa', 'sa_sesotho'], 512, 3000, 600, seconds=4, skip_augment=True)\n",
    "[849, 351, 1200, 0.7075, 0.2925]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "many_languages = ['ab_English', 'ab_German', 'ab_Russian', 'khmer', 'nepali', 'sa_afrikaans', 'sa_sesotho']\n",
    "gexp6 = GMMExperiment(many_languages, 512, 2000, 500, seconds=3, skip_augment=False)\n",
    "[666, 2834, 3500, 0.19028571428571428, 0.8097142857142857]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "big_languages = ['ab_English', 'ab_German', 'ab_Russian']\n",
    "gexp11 = GMMExperiment(big_languages, 512, 3000, 500, seconds=9, skip_augment=False)\n",
    "[552, 948, 1500, 0.368, 0.632]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "big_languages = ['ab_English', 'ab_German', 'ab_Russian']\n",
    "gexp13 = GMMExperiment(big_languages, 512, 3000, 500, seconds=9, skip_augment=True)\n",
    "[520, 980, 1500, 0.3466666666666667, 0.6533333333333333]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "big_languages_2 = ['ab_English', 'ab_German', 'ab_Russian', 'ab_Hebrew', 'ab_Danish']\n",
    "gexp16 = GMMExperiment(big_languages_2, 512, 2000, 400, seconds=9, skip_augment=False)\n",
    "[408, 1592, 2000, 0.204, 0.796]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
