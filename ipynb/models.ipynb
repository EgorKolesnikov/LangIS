{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import IPython.display\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from random import shuffle\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from keras.utils import np_utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def read_png(path):\n",
    "    img = mpimg.imread(path)\n",
    "    return rgb2gray(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature File Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramFeatureFileManager(object):\n",
    "    def __init__(self, seconds=10, skip_augment=False):\n",
    "        self.seconds = seconds\n",
    "        self.skip_augment = skip_augment\n",
    "\n",
    "    def get_file_credentials(self, path):\n",
    "        values = path.split('#')[-1].split('.')[0]\n",
    "        sr, winlen, winstep = map(int, values.split('='))\n",
    "        return sr, winlen, winstep\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\" Return list of objects, ectracted from given file \"\"\"\n",
    "        if self.skip_augment and u'#AUG#' in path:\n",
    "            return []\n",
    "\n",
    "        file_data = read_png(path)\n",
    "\n",
    "        sr, winlen, winstep = self.get_file_credentials(path)\n",
    "        one_sec_count = sr / winstep\n",
    "        chunk_size = one_sec_count * self.seconds\n",
    "\n",
    "        height, width = file_data.shape\n",
    "        \n",
    "        # print file_data.shape, width, sr, winlen, winstep\n",
    "        # print one_sec_count, chunk_size\n",
    "        \n",
    "        if width < chunk_size:\n",
    "            return []\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        chunks = (width + chunk_size - 1) / chunk_size\n",
    "        for chunk in xrange(chunks):\n",
    "            start = (chunk * chunk_size)\n",
    "            end = min(file_data.shape[1], (chunk + 1) * chunk_size)\n",
    "            \n",
    "            if end - start < chunk_size:\n",
    "                break\n",
    "\n",
    "            result.append(file_data[:, start:end])\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/kolegor/Study/Master/Work/data/features/spectrogram/ab_Russian/train/Leo-Tolstoy-Detstvo-RUSSIAN-24-V_posteli_64kb.mp3.wav.chunk018.png#24000=360=600.png'\n",
    "# manager = SpectrogramFeatureFileManager(seconds=3)\n",
    "# asd = manager.load(path)\n",
    "# len(asd)\n",
    "# librosa.display.specshow(read_png(path), sr=22050, x_axis='time', y_axis='mel')\n",
    "# plt.show()\n",
    "# for a in asd:\n",
    "#     librosa.display.specshow(a, sr=22050, x_axis='time', y_axis='mel')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FolderIterator(object):\n",
    "    PATH_TEMPLATE = '/home/kolegor/Study/Master/Work/data/features/spectrogram/{language}/{dataset}/'\n",
    "\n",
    "    def __init__(self, language, dataset, uid):\n",
    "        self.language = language\n",
    "        self.dataset = dataset\n",
    "        self.uid = uid\n",
    "\n",
    "        self.path = self.PATH_TEMPLATE.format(language=language, dataset=dataset)\n",
    "    \n",
    "    def get_data(self):\n",
    "        x = [os.path.join(self.path, filename) for filename in os.listdir(self.path)]\n",
    "        y = [self.uid for _ in xrange(len(x))]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, folder_iterators, need_shuffle=True, manager=None):\n",
    "        self.folders = folder_iterators\n",
    "        \n",
    "        self.manager = manager\n",
    "        if self.manager is None:\n",
    "            self.manager = SpectrogramFeatureFileManager(\n",
    "                seconds=5,\n",
    "                skip_augment=False\n",
    "            )\n",
    "        \n",
    "        self.size_by_language = dict()\n",
    "        \n",
    "        self.all_x = []\n",
    "        self.all_y = []\n",
    "        \n",
    "        for folder in self.folders:\n",
    "            x, y = folder.get_data()\n",
    "            if len(x) != len(y):\n",
    "                raise Exception('What the hell?')\n",
    "            \n",
    "            self.all_x.extend(x)\n",
    "            self.all_y.extend(y)\n",
    "            self.size_by_language[folder.language] = len(x)\n",
    "        \n",
    "        if shuffle:\n",
    "            combined = zip(self.all_x, self.all_y)\n",
    "            shuffle(combined)\n",
    "            self.all_x, self.all_y = zip(*combined)\n",
    "        \n",
    "        self.classes = len(set(self.all_y))\n",
    "        self.size = len(self.all_x)\n",
    "\n",
    "    def one_sample(self):\n",
    "        qwe = []\n",
    "        while not qwe:\n",
    "            qwe = self.manager.load(self.all_x[0])\n",
    "            if qwe:\n",
    "                qwe = qwe[0]\n",
    "                break\n",
    "        return qwe\n",
    "\n",
    "\n",
    "def dataset_iterator(dataset, chunk_size, as_np=False, extend=True, y_categorical=True):\n",
    "    cur_chunk_x = []\n",
    "    cur_chunk_y = []\n",
    "\n",
    "    for filepath, uid in zip(dataset.all_x, dataset.all_y):\n",
    "        loaded = dataset.manager.load(filepath)\n",
    "\n",
    "        cur_chunk_x.extend(loaded)\n",
    "        cur_chunk_y.extend([uid for _ in xrange(len(loaded))])\n",
    "\n",
    "        if len(cur_chunk_x) >= chunk_size:\n",
    "            to_return_x = cur_chunk_x[:chunk_size]\n",
    "            to_return_y = cur_chunk_y[:chunk_size]\n",
    "\n",
    "            cur_chunk_x = cur_chunk_x[chunk_size:]\n",
    "            cur_chunk_y = cur_chunk_y[chunk_size:]\n",
    "            \n",
    "            if as_np:\n",
    "                to_return_x = np.array(to_return_x)\n",
    "                to_return_y = np.array(to_return_y)\n",
    "\n",
    "            if extend:\n",
    "                to_return_x = to_return_x.reshape(to_return_x.shape + (1,))\n",
    "            \n",
    "            if y_categorical:\n",
    "                to_return_y = np_utils.to_categorical(to_return_y, dataset.classes)\n",
    "\n",
    "            yield to_return_x, to_return_y\n",
    "\n",
    "\n",
    "def load_all_dataset(dataset, as_np=False, extend=True, y_categorical=True):\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "    \n",
    "    iterator = dataset_iterator(dataset, 32, as_np=False, extend=False, y_categorical=False)\n",
    "    for x, y in iterator:\n",
    "        all_x.extend(x)\n",
    "        all_y.extend(y)\n",
    "        \n",
    "    all_x = np.array(all_x)\n",
    "    all_y = np.array(all_y)\n",
    "    \n",
    "    if as_np:\n",
    "        all_x = np.array(all_x)\n",
    "        all_y = np.array(all_y)\n",
    "\n",
    "    if extend:\n",
    "        all_x = all_x.reshape(all_x.shape + (1,))\n",
    "\n",
    "    if y_categorical:\n",
    "        all_y = np_utils.to_categorical(all_y, dataset.classes)\n",
    "\n",
    "    return all_x, all_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_languages_dataset(languages):\n",
    "    language_to_uid = dict((b, a) for a, b in enumerate(languages))\n",
    "    train = Dataset([FolderIterator(language, 'train', language_to_uid[language]) for language in languages])\n",
    "    test = Dataset([FolderIterator(language, 'test', language_to_uid[language]) for language in languages])\n",
    "    dev = Dataset([FolderIterator(language, 'dev', language_to_uid[language]) for language in languages])\n",
    "    return train, test, dev, language_to_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_languages_info(train_ds, test_ds, dev_ds):\n",
    "    print u'Languages: [{}]'.format(u' # '.join(train_ds.size_by_language.keys()))\n",
    "    print u'Classes: {}'.format(train_dataset.classes)\n",
    "    print u'Sizes: (train/test/dev): {}/{}/{}'.format(train_ds.size, test_ds.size, dev_ds.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: [ab_Russian # ab_Portuguese]\n",
      "Classes: 2\n",
      "Sizes: (train/test/dev): 16492/4713/2357\n"
     ]
    }
   ],
   "source": [
    "languages = ['ab_Portuguese', 'ab_Russian']\n",
    "train_dataset, test_dataset, dev_dataset, language_to_uid = get_languages_dataset(languages)\n",
    "print_languages_info(train_dataset, test_dataset, dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_X, dev_Y = load_all_dataset(dev_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(train_ds):\n",
    "    sample = train_ds.one_sample()\n",
    "    input_shape = (sample.shape[0], sample.shape[1], 1)\n",
    "    print input_shape\n",
    "    \n",
    "    pool_size=(2, 2)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (7, 7), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "    \n",
    "    # print model.summary()\n",
    "    # model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(train_ds.classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 200, 1)\n"
     ]
    }
   ],
   "source": [
    "model = get_model(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, X_train=None, Y_train=None, X_dev=None, Y_dev=None):\n",
    "    batch_size = 32\n",
    "    epochs = 4\n",
    "    \n",
    "    if X_train is None and Y_train is None:\n",
    "        print 'Loading train data'\n",
    "        X_train, Y_train = load_all_dataset(train_dataset)\n",
    "    \n",
    "    if X_dev is None and Y_dev is None:\n",
    "        print 'Loading dev data'\n",
    "        X_dev, Y_dev = load_all_dataset(dev_dataset)\n",
    "\n",
    "    return model.fit(\n",
    "        x=X_train,\n",
    "        y=Y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_dev, Y_dev),\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_generator(model):\n",
    "    batch_size = 32\n",
    "    epochs = 4\n",
    "\n",
    "    model.fit_generator(\n",
    "        dataset_iterator(train_dataset, batch_size, as_np=True, extend=True),\n",
    "        steps_per_epoch=len(train_dataset.all_x) / batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=dataset_iterator(dev_dataset, batch_size, as_np=True, extend=True),\n",
    "        validation_steps=len(dev_dataset.all_x) / batch_size,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 'Loading train data'\n",
    "# X_train, Y_train = load_all_dataset(train_dataset)\n",
    "# print 'Loading dev data'\n",
    "# X_dev, Y_dev = load_all_dataset(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "515/515 [==============================] - 4374s 8s/step - loss: 0.3613 - acc: 0.8217 - val_loss: 3.1376 - val_acc: 0.4041\n",
      "Epoch 2/4\n",
      "328/515 [==================>...........] - ETA: 26:20 - loss: 0.1646 - acc: 0.9346"
     ]
    }
   ],
   "source": [
    "# _ = fit(model, X_train=X_train, Y_train=Y_train, X_dev=X_dev, Y_dev=Y_dev)\n",
    "_ = fit_generator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
